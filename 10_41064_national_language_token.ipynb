{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a107a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ส่วน Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d246f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1784c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e4b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input text\n",
    "i_text = \"Do you know how to tokenization works? It's actually quite intereting! Let's analyse a couple of sentences and figure it out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2051151",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6d5c4d4fe17a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sentence tokenization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "print(sent_tokenize(i_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenization\n",
    "print(word_tokenize(i_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word Punctuation tokenization\n",
    "print(WordPunctTokenize().token(i_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50682dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a822612",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_word = ['writing','calves','be','branded','horse','randomize',\n",
    "         'possibly','provision','hospital','kept','scratchy','code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0986e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create various stemmer object\n",
    "porter=PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "snowball=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "198c608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "#create a list of stemmer names for display\n",
    "stemmer_names=['PORTER','LANCASTER','SNOWBALL']\n",
    "formatted_text='{:>16}' * (len(stemmer_names) +1)\n",
    "print('\\n',formatted_text.format('INPUT WORD',*stemmer_names),\n",
    "         '\\n','='*68)\n",
    "\n",
    "#stem each word and display the ouput\n",
    "for word in i_word:\n",
    "    output=[word,porter.stem(word),\n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Lemmatizer object (แยก adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f07145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9353a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first 12000 words from the Brown corpus\n",
    "input_data = ' '.join(brown.words()[:12000])\n",
    "\n",
    "# Define the number of words in each chunk \n",
    "chunk_size = 700\n",
    "\n",
    "chunks = chunker(input_data, chunk_size)\n",
    "print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk', i+1, '==>', chunk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ccd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ส่วน processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805d279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown #standard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import brown\n",
    "from text_chunker import chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the Brown corpus\n",
    "input_data = ' '.join(brown.words()[:5400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca175b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = chunker(input_data, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899516b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dict items\n",
    "chunks = []\n",
    "for count, chunk in enumerate(text_chunks):\n",
    "    d = {'index': count, 'text': chunk}\n",
    "    chunks.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the document term matrix\n",
    "count_vectorizer = CountVectorizer(min_df=7, max_df=20)\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b39663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocabulary and display it\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names())\n",
    "print(\"\\nVocabulary:\\n\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate names for chunks\n",
    "chunk_names = []\n",
    "for i in range(len(text_chunks)):\n",
    "    chunk_names.append('Chunk-' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the document term matrix\n",
    "print(\"\\nDocument term matrix:\")\n",
    "formatted_text = '{:>12}' * (len(chunk_names) + 1)\n",
    "print('\\n', formatted_text.format('Word', *chunk_names), '\\n')\n",
    "for word, item in zip(vocabulary, document_term_matrix.T):\n",
    "    # 'item' is a 'csr_matrix' data structure\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c476711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the category map\n",
    "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos', \n",
    "        'rec.sport.hockey': 'Hockey', 'sci.electronics': 'Electronics', \n",
    "        'sci.med': 'Medicine'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training dataset\n",
    "training_data = fetch_20newsgroups(subset='train', \n",
    "        categories=category_map.keys(), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a count vectorizer and extract term counts \n",
    "count_vectorizer = CountVectorizer()\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print(\"\\nDimensions of training data:\", train_tc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test data \n",
    "input_data = [\n",
    "    'You need to be careful with cars when you are driving on slippery roads', \n",
    "    'A lot of devices can be operated wirelessly',\n",
    "    'Players need to be careful when they are close to goal posts',\n",
    "    'Political debates help us understand the perspectives of both sides'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "\n",
    "# Transform input data using count vectorizer\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "\n",
    "# Transform vectorized data using tfidf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "# Predict the output categories\n",
    "predictions = classifier.predict(input_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the outputs\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print('\\nInput:', sent, '\\nPredicted category:', \\\n",
    "            category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bd1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000a6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db24ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
